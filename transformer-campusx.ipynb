{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"markdown","source":"Transformer is a neural network structure\n\njust like - \n\nann - best for tabular data\n\ncnn - best for pictorial data\n\nrnn - best for sequential data like text\n\ntransformer - best for sequence to sequence task","metadata":{}},{"cell_type":"markdown","source":"# sequence to sequence task","metadata":{}},{"cell_type":"markdown","source":"input - a sequence\n\noutput -also an sequence\n\nexample like - \n\nmachine translation (converting one sentence from one language to other)\n\nquestion answering system\n\ntext summarisation","metadata":{}},{"cell_type":"markdown","source":"Transformer also has an encoder and decoder like architecture (like used in previous seqtoseq models)\n\nbut it does not use lstm but use a new method called as self attention\n\nit can work in parallel thus it is very scalable","metadata":{}},{"cell_type":"markdown","source":"# Impact of transformers","metadata":{}},{"cell_type":"markdown","source":"1. Revolution in NLP - give state of the arts results\n\n1. Transfer learning in textual data - as with transformer large models like bert ang gpt can be trained and can be fine tuned to \n   perform specific task\n \n1. Multimodal capacity -transformer architecture is flexible enough for diffrent types of input and output\n\n1. Acceleration of generative ai\n\n1. Unification of deep learning - a single architecture can be used in almost all type of problems","metadata":{}},{"cell_type":"markdown","source":"# Origin story","metadata":{}},{"cell_type":"markdown","source":"Transformer is build on three research paper -","metadata":{}},{"cell_type":"markdown","source":"# Sequence to Sequence learning with neural networks - ","metadata":{}},{"cell_type":"markdown","source":"Architecture - \n\nUsed for machine translation task\n\nuses encoder decoder architecture\n\nuses lstm\n\nencoder outputs a single output called as context vector (it provides summary of text inputted on encoder (encodings) to decoder)\n\nworks good for input text of size < 30 words\n\nproblem - as words increase it is not possible to pack that much information in context vector (information overload)","metadata":{}},{"cell_type":"markdown","source":"# Neural machine translation by jointly learning to align and translate","metadata":{}},{"cell_type":"markdown","source":"proposed concept of attention\n\nencoder works the same as previous\n\nno single context vector as previous\n\ncontext vector for every generated word in decoder is different.\n\nit depends upon weighted sum of every hidden state of encoder\n\nthe weights(attention weights) are calculated dynamically\n\nworks good for >30 words\n\nuses lstm (thus not scalable because work in sequential manner)\n\ncant train very large data (thus cant make big model capable of transfer learning like in cnns)","metadata":{}},{"cell_type":"markdown","source":"# Attention is all you need","metadata":{}},{"cell_type":"markdown","source":"it also uses encoder decoder architecture\n\nreplaces lstm with self attention\n\nit is very scalable as parallel processing is possible","metadata":{}},{"cell_type":"markdown","source":"# Advantages of transformer - ","metadata":{}},{"cell_type":"markdown","source":"scalability\n\ntransfer learning\n\nmultimodal capacity\n\nFlexible architecture - diffrent type of trnasformer like encoder only or decoder only (as per task requirement)\n\necosystem - large resources (ex -Hugging Face)\n\nAI Technique Integration - ex- gans + transformer,cnn + transformer ,reinforcement learning +transformer","metadata":{}},{"cell_type":"markdown","source":"# Real world Examples -","metadata":{}},{"cell_type":"markdown","source":"chatgpt\n\ndalle-2\n\nAlphaFold\n\nOpenAI Codex\n\nA Comprehensive Survey on Applications of Transformers for Deep Learning Tasks:\n\nhttps://arxiv.org/abs/2306.07303","metadata":{}},{"cell_type":"markdown","source":"# Disadvantages -","metadata":{}},{"cell_type":"markdown","source":"high computation\n\nlarge data for specific work\n\nenergy consumption\n\ninterpretability - black box model\n\nbias\n\nethical concern","metadata":{}},{"cell_type":"markdown","source":"# Future work - ","metadata":{}},{"cell_type":"markdown","source":"improving efficiency - better performance\n\nmultimodal capabilities - ex (sensory data, biometric data, time series)\n\nresponsible development - sustainable and legal\n\ndomain specific - ex - for school ,for laws etc\n\nreasonal based - in all languages\n\ninterpretability - we will understand working","metadata":{}}]}